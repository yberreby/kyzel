from enum import Enum, auto
from transformers import LogitsProcessor
from torch import Tensor, FloatTensor
import logging
from typing import List, Dict, Tuple, Optional

from .logit_utils import force_token

logger = logging.getLogger(__name__)

# Verbatim start of a code block. This is a constant.
code_start = "```python\n"


class State(Enum):
    START = auto()
    THOUGHT_CONTENT = auto()
    ACTION_OPEN = auto()
    ACTION_CONTENT = auto()
    CODE_FENCE_START = auto()
    CODE_CONTENT = auto()
    DONE = auto()


def get_code_block_status(text: str) -> Tuple[bool, bool]:
    """
    Cares about Markdown code blocks.

    Returns (has_content, should_end)
    """

    # Should be improved so that we check that *there is a line that starts with `code_start`*
    # Would be more robust. Fine for now...
    if not text or code_start not in text:
        return False, False

    # Also brittle.
    start_loc = text.find(code_start)

    # Ok, so we found a code block. Let's care about the end.
    lines = text[start_loc:].split("\n")[1:]  # Skip start line (start fence)
    for i, line in enumerate(lines):
        # No leading whitespace allowed!
        begins_with_triple_backticks = line.startswith("```")
        trailing_chars = line[3:].strip()
        if begins_with_triple_backticks and not trailing_chars:
            # Any nonempty line?
            has_content = any(l.strip() for l in lines[:i])
            return has_content, True

    return True, False


def get_next_state(state: State, text: str) -> Tuple[State, Optional[str]]:
    """
    Returns (new_state, forced_sequence)
    """
    match state:
        case State.START:
            return State.THOUGHT_CONTENT, "<thought>"

        case State.THOUGHT_CONTENT if "</thought>" in text:
            return State.ACTION_OPEN, "\n<action>"

        case State.ACTION_OPEN:
            return State.ACTION_CONTENT, None

        case State.ACTION_CONTENT if "</action>" in text:
            return State.CODE_FENCE_START, "\n" + code_start

        case State.CODE_FENCE_START:
            return State.CODE_CONTENT, None

        case State.CODE_CONTENT:
            has_content, should_end = get_code_block_status(text)
            if has_content and should_end:
                return State.DONE, "\n"

    return state, None


class StructuredEnforcer(LogitsProcessor):
    def __init__(self, tokenizer):
        assert hasattr(tokenizer, "eos_token_id") and tokenizer.eos_token_id is not None

        self.tokenizer = tokenizer
        self.eos_token_id = tokenizer.eos_token_id

        # State of our state machine.
        self.state = State.START

        # The position, as a token index, of the first token generated after this instance was created.
        self.start_pos = None

        # If any, pending tokens whose generation we want to force.
        self._tokens_to_force = []

        # For debugging / the future.
        self._token_history: List[Tuple[int, str]] = []  # [(id, text)]

    def _pop_forced(self) -> Optional[int]:
        """
        Utility function to pop the next token to force, if any.
        """
        if not self._tokens_to_force:
            return None
        return self._tokens_to_force.pop(0)

    def __call__(self, input_ids: Tensor, scores: FloatTensor) -> FloatTensor:
        # Initialize start position if needed
        # A given instance will be called many times, with growing `input_ids`.
        # The very first `input_ids` give us the true starting point.
        if self.start_pos is None:
            self.start_pos = input_ids.shape[1]
            logger.debug(f"Generation starts at position {self.start_pos}")

        # By default, EOS is forbidden.
        scores[0, self.eos_token_id] = float("-inf")

        # All tokens generated since the first __call__
        # This will be empty in the first __call__!... Handle that.
        if generated_tokens := input_ids[0, self.start_pos :].tolist():
            # For debugging.
            new_token = generated_tokens[-1]
            assert new_token is not None
            self._log_new_token(new_token)

            # All text generated by the assistant so far.
            generated_text = self.tokenizer.decode(generated_tokens)
        else:
            # First __call__.
            generated_text = ""

        # If we have a "forced sequence", we're completely overriding the model's behavior temporarily.
        # This is a "whitelist" rather than a "blacklist" approach.
        # Most of the time, we want to eliminate illegal tokens (and let the
        # model do what its want for legal ones), but in forcing, we want to
        # artificially insert tokens that we know should be there.
        # We do this, as usual, through the logits.
        #
        # We'll hit this branch in multiple __call__s, as we force one token at a time,
        # until the sequence is exhausted.
        if next_forced_token := self._pop_forced():
            return force_token(scores, next_forced_token)

        # Given the current state and the text generated so far, what should we do next?
        new_state, string_to_force = get_next_state(self.state, generated_text)
        if new_state != self.state:
            logger.debug(f"State transition: {self.state} -> {new_state}")
            self.state = new_state

            if string_to_force:
                # Can't encode nothing.
                self._tokens_to_force = self.tokenizer.encode(
                    string_to_force, add_special_tokens=False
                )

            # Reevaluate our logit forcing in light of the new state.
            return self.__call__(input_ids, scores)

        # Force EOS if done: fully deterministic.
        # Note that if this were at the beginning of the function, we'd get an extra token...
        if self.state == State.DONE:
            return force_token(scores, self.eos_token_id)

        return scores

    def _log_new_token(self, new_token: int):
        """
        Just there for debugging.
        """
        token_text = self.tokenizer.decode([new_token])
        self._token_history.append((new_token, token_text))
        logger.debug(f"Token {len(self._token_history)}: {new_token} -> {token_text!r}")
