<session>
<events>
<msg from="user">
Get my org-roam database, embed each node title with a small SBERT, UMAP-reduce to 2 dim and plot to a file
</msg>

<msg from="assistant">
Ah, you want to visualize the semantic layout of your org-roam DB's titles!

# High-level plan
We will need to:
1. Find the file and read node titles from it
2. Compute SBERT embeddings from short strings (locally, I assume)
3. Reduce the embeddings to 2D with UMAP
4. Plot the results

# Detailed breakdown

## Step 1 - Find file and read titles
- Locate the org-roam DB file
- There are a few standard locations we could try
- But we might also get it by running `emacsclient --eval "org-roam-db-location"`
- It's a SQLite file; we can parse it with `sqlite3`
- Once we can run queries, let's have a quick look at the schema and the available data

## Step 2 - Embedding computation
- Verify that we can compute SBERT embeddings 1) reasonably *fast* 2) within reasonable *memory*
- "Reasonable memory" depends on 1) total physical RAM 2) *other processes* running; might need to ask the user to free some RAM
- "small SBERT" -> performance likely a concern -> use batching to the extent possible (depends on available memory)
- But probably start with just 1 element at a time

## Step 3 - Dimensionality reduction
- UMAP is a complex algorithm. We don't want to reinvent the wheel.
- PyPI package: `umap-learn`
- *Not super standard -> available? Need to install?*

## Step 4 - Plotting
- `matplotlib` is a reasonable default.
- *2D scatter plot* seems good.
- We must be able to *see which title corresponds to which point*.
- Add a few random annotations to avoid overcrowding.
- Save to file

# Critical information to gather

Let's start by:
- Querying total and available memory
- Importing `numpy`, `matplotlib`, `pandas`, `sqlite3`, `umap-learn` - ensure that they are available
- Trying the `emacsclient` command to get the DB location - if it works, that's the easiest way to get the DB path

On to the first small task...
</msg>

<action>Checking total and available system memory</action>
<code>
import psutil
mem = psutil.virtual_memory()
print(f"Total memory: {mem.total / 1e9:.1f}GB")
print(f"Available memory: {mem.available / 1e9:.1f}GB")
print(f"Memory usage: {mem.percent}%")
</code>

<result>
Total memory: 32.0GB
Available memory: 24.5GB
Memory usage: 23.4%
</result>
<thought>Memory seems sufficient. We can proceed with imports, from most basic to most specialized.</thought>

<action>Importing essential packages</action>
<code>
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sqlite3
import umap
import sentence_transformers
</code>

<result />
<thought>All available, great.</thought>


<action>Querying the org-roam DB location using emacsclient.</action>
<code>
import subprocess

try:
    db_path = subprocess.check_output(
        ['emacsclient', '--eval', 'org-roam-db-location'],
        stderr=subprocess.PIPE,
        text=True
    ).strip()
    print(f"Found DB path: {db_path}")
except subprocess.CalledProcessError as e:
    print(f"Error running emacsclient: {e.stderr}")
</code>
<result>
Error running emacsclient: emacsclient: can't find socket; have you started the server?
emacsclient: To start the server in Emacs, type "M-x server-start".
emacsclient: No socket or alternate editor.  Please use:

	--socket-name
	--server-file      (or environment variable EMACS_SERVER_FILE)
	--alternate-editor (or environment variable ALTERNATE_EDITOR)
</result>
<thought>Looks like there's no server running. Change of plans...</thought>

<action>Looking for org-roam.db in standard locations</action>
<code>
from pathlib import Path
import os

potential_paths = [
    Path.home() / '.emacs.d' / 'org-roam.db',
    Path.home() / '.doom.d' / 'org-roam.db',
    Path.home() / '.config' / 'emacs' / 'org-roam.db'
]

for path in potential_paths:
    if path.exists():
        print(f"Found DB: {path}")
        db_path = path
        break
else:
    print("Could not find org-roam.db in standard locations")
</code>
<result>
Found DB: /home/yohai/.doom.d/org-roam.db
</result>
<thought>Finally, found it.</thought>

<action>Ensuring we can load the DB, looking at available tables</action>
<code>
conn = sqlite3.connect(db_path)
cur = conn.cursor()

# First, let's look at the schema
cur.execute("SELECT name FROM sqlite_master WHERE type='table';")
print("Tables in database:", cur.fetchall())
</code>
<result>
Tables in database: [('files',), ('nodes',), ('aliases',), ('citations',), ('refs',), ('tags',), ('links',)]
</result>
<thought>Looks like the file is a valid SQLite DB. We're interested in the `nodes` table.</thought>


<action>Inspecting the schema of the `nodes` table</action>
<code>
table_name = 'nodes'
cur.execute(f"PRAGMA table_info({table_name})")

schema_info = cur.fetchall()
for column in schema_info:
    print(column)
</code>
<result>
(0, 'id', '', 1, None, 1)
(1, 'file', '', 1, None, 0)
(2, 'level', '', 1, None, 0)
(3, 'pos', '', 1, None, 0)
(4, 'todo', '', 0, None, 0)
(5, 'priority', '', 0, None, 0)
(6, 'scheduled', 'TEXT', 0, None, 0)
(7, 'deadline', 'TEXT', 0, None, 0)
(8, 'title', '', 0, None, 0)
(9, 'properties', '', 0, None, 0)
(10, 'olp', '', 0, None, 0)
</result>
<thought>Seems reasonable. We mostly care about the `title` column, here.</thought>

<action>Getting all node titles from the `nodes` table</action>
<code>
cur.execute("SELECT title FROM nodes;")
titles = [row[0] for row in cur.fetchall() if row[0]]
print(f"Found {len(titles)} titles")
print("Sample titles:", titles[:5])
</code>
<result>
Found 3416 titles
Sample titles: ['"2024-05-31"', '"2024-05-29"', '"2024-05-30"', '"2024-05-25"', '"2024-05-27"']
</result>
<thought>That's a lot of nodes. We'll have to be careful when processing that much data. Also - what's up with those nested quotes? This might be problematic later on.</thought>


<action>Cleaning titles by removing outer quotes if present</action>
<code>
titles_cleaned = [t[1:-1] if t.startswith('"') and t.endswith('"') else t for t in titles]
print("Sampled cleaned titles:")
print("  Beginning:", titles[:5])
print("  End:", titles[-5:])
</code>
<result><![CDATA[
Sampled cleaned titles:
  Beginning: ['2024-05-31', '2024-05-29', '2024-05-30', '2024-05-25', '2024-05-27']
  End: ['2025-01-18', 'Orthogonal complement', 'Schur decomposition', 'Common Taylor expansions', 'My: context for LLMs']
]]></result>
<thought>Seems to have worked. Now that we have clean titles, we can embed them --- but first, we need to load an embedding model. This tends to be an expensive operation, so we'll time it.</thought>

<action>Loading SBERT model: `all-MiniLM-L6-v2`</action>
<code>
%%time
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
# Can't hurt to have a look inside.
model
</code>
<result>
CPU times: user 57.5 ms, sys: 16.6 ms, total: 74.1 ms
Wall time: 437 ms
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
</result>
<thought>Seems to have worked. Not instantaneous, but reasonably fast. Interestingly, the max sequence length is 256. Might become important. We should truncate the titles to a reasonable length - maybe 100 characters.</thought>


<action>Truncating titles to 100 characters</action>
<code>
max_title_len = 100
titles_truncated = [t[:max_title_len] for t in titles_cleaned]
</code>
<result/>

<thought>No issue here. Now, we'll embed them. Let's use a batch size of 32 to strike a balance between memory usage and speed.</thought>

<action>Computing embeddings of cleaned, truncated titles</action>
<code>
embeddings = model.encode(
    titles_truncated,
    show_progress_bar=False,
    batch_size=32
)
print(f"Computed embeddings: {embeddings.shape}")
</code>
<result><![CDATA[
Computed embeddings: (3416, 384)
]]></result>
<thought>We've got our embeddings, in a `(num_samples, embed_dim)` tensor. Next, dimensionality reduction.</thought>

<action>Reducing dimensionality from 384 to 2 with UMAP</action>
<code>
reducer = umap.UMAP(n_components=2, random_state=42)
coords = reducer.fit_transform(embeddings)
print(f"Reduced to 2D: {coords.shape}")
</code>
<result>
Reduced to 2D: (3416, 2)
</result>
<thought>Next up: plotting.</thought>

<action>Creating scatter plot</action>
<code>
plt.figure(figsize=(12, 8))
plt.scatter(coords[:, 0], coords[:, 1], alpha=0.5, s=10)
plt.title('Org-Roam Notes Semantic Layout')
plt.xlabel('UMAP 1')
plt.ylabel('UMAP 2')
</code>
<result/>
<thought>Let's improve the plot before saving. Points without labels are not very meaningful, but we should be careful about overcrowding, since we have a lot of points. Let's only annotate a random subset of the points.</thought>

<thought>Using `plt.annotate` on a few points.</thought>
<code>
n_annotations = 20
indices = np.random.choice(len(titles_truncated), size=n_annotations, replace=False)

for i in indices:
    plt.annotate(titles_truncated[i],
                (coords[i, 0], coords[i, 1]),
                xytext=(5, 5), textcoords='offset points',
                fontsize=8)
</code>
<result/>

<action>Saving the final plot to `./org_roam_map.png`</action>
<code>
plt.tight_layout()
plt.savefig('org_roam_map.png', dpi=300, bbox_inches='tight')
</code>
<result/>

</events>
</session>
